<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Reinforcement Learning for Relation Classification from Noisy Data]]></title>
    <url>%2F2017%2Fpaper_1_RL%2F</url>
    <content type="text"><![CDATA[这篇文章是Reinforcement Learning在NLP方面的应用，具体是用在有噪音训练集的实体关系分类问题上，已经被AAAI 2018录用，作者之前也在PaperWeekly分享过了，最近一个前同事提到Reinforcement Learning应该会是NLP的下一个热点，想想应该也是这么回事，就像之前的Attention机制一样。因此在了解了Reinforcement Learning的基本概念之后，又把这篇文章细读一遍。链接：Reinforcement Learning for Relation Classification from Noisy Data 摘要现有的关系分类方法主要依赖于距离监督（distant supervision），主要的假设是假如有一组句子都提到了一个实体对，那么这些句子就都描述了这个实体对的一个关系（我觉得可能就是和共现差不多吧）。这些方法一般都是在句子集的层面进行分类，而不能把关系和句子一一对应起来，造成这个问题的主要原因是标注数据集里有噪声数据。作者在这篇文章里提出了一种新的方法，这种方法可以在有噪声的数据上做到句子层面的关系分类。这个模型有两个组成部分：实例选取器（instance selector）和关系分类器（relation classifier）。实例选取器用强化学习的方法选取高质量的句子，并传递给关系分类器，关系分类器在句子层面作出预测，并反馈给实例选取器。这两个模块共同训练、优化。实验结果表明这样的方法可以有效处理训练数据中的噪声，从而在句子层面的关系分类上取得更好的performance。 简介为解决标注数据含噪声的问题，现有的Bag Level的关系分类方法有两个缺陷：1. 不能处理句子级别的关系分类； 2. 假如‘Bag’中的所有句子都是噪声句子，对模型的影响很大。下图是对第一个缺陷的解释说明，‘Bag-Level’也就是relation的label是给Bag的，这一组句子包含同一个实体对，描述了哪些关系，而‘Sentence-Level’的relation label是给Sentence的，也就是对每一个句子指定一个关系类型。造成第二个缺陷的原因是，即使‘Bag’中所有句子都是噪声（也就是这些句子都没有描述某个关系），模型也会选取至少一个句子，认为选取的句子描述了某个关系，进而基于这些句子进行模型训练，而且假如标注集只是简单基于距离监督得到，这样的情况是很常见的，因此这样训练得到的分类模型的performance肯定会下降。 为了解决上述的两个缺陷，作者提出加入实例选取器，并将其转换成一个强化学习问题来解决。因为这个选取器有以下两个特征：首先，这个句子选取是一个反复试错（trial-and-error-search）的过程，需要从分类器得到对选取的句子质量的反馈（reward）；其次，上述的反馈只能在整个挑选过程结束之后才能得到，因此反馈是滞后（delayed）的。这两点都非常符合强化学习的特点。 模型 上图是整个模型的框架，由Instance Selector和Relation Classifier两部分组成，下面具体介绍两部分。 Instance Selector因为Instance Selector是当作强化学习问题处理，因此policy function的更新会滞后，为了加快更新速度，作者采用了把所有句子分成$N$个Bag的做法，即$B={B^1, B^2,\cdots, B^N}$。对于每个$B^k$，都包含同一个实体对，而且标注的关系都是$r^k$，虽然这个标注可能是有噪声的（不准确的）。之后的训练都是以一个Bag为单位进行操作。下面就要介绍作为强化学习，State、Action、Reward的定义。 State State $s_i$ 包含了以下三部分信息：1.当前句子的向量表示，这个向量表示由关系分类器里CNN的非线性输出层得到；2.被选的句子集合的矩阵表示，这个由被选句子的向量表示取平均得到；3.句子中实体对的矩阵表示，这个从预训练的知识图谱embeddings得到，这个embeddings是在Freebase上采用TransE模型训练得到。 Action action $a_i \in {0, 1}$表示是否应该选取第$i$个句子，$a_i$的取值由policy function得到，具体定义如下：$\pi_\Theta(s_i, a_i) = a_i\sigma(W*F(s_i)+b)+(1-a_i)(1-\sigma(W*F(s_i)+b))$这里的$\sigma(.)$是sigmoid函数。 Reward reward由基于CNN的分类器反馈得到，计算所选的句子集合$\hat B$的likelihood，假设所选的句子集合是空集，则用所有句子的平均likelihood来计算，这应该是文章能够有效选出噪声数据的关键之一。 Relation Classifier文章用的分类模型比较简单，也是目前效果比较好的CNN，包括输入层，卷积层，max-pooling层。 输入层 对于每个词的表示分为两部分，一部分是word2vec训练得到的embedding，另一部分是position embedding，这个position embedding其实是两个固定长度的向量，分别用来表示某个词到关系里的两个实体的距离，具体可以参考中科院这篇文章。 参数设置 para value word embedding 50 position embedding 5 learning rate 0.02 dropout 0.5 convolution window 3 试验 数据集 数据集来自New York Times (NYT)，里面包含了39528个不重复的实体和53个不重复的关系类别，具体数据如下： sentence entity pairs relation facts training set 522611 281270 18252 test set 172448 96678 1950 评价方式 因为本身数据集是有噪音的，所以作者随机选取了300个句子，人工标注，然后在这些句子上做评测，评测包括Accuracy和Macro F-value。设定的baseline包括：CNN、CNN+Max（每个bag里选一个认为正确的句子）、CNN+ATT（加入attention机制，降低噪声数据的权重）。 训练过程ALGORITHM 1描述的是整体训练过程，也就是先预训练CNN分类模型，使得$log p(r_i|x_i)$最大，然后固定CNN分类器，预训练policy network，最后联合训练两个模型直至收敛。ALGORITHM 2描述的是具体怎么实现联合训练，对于每一次的epoch，分为N个word bag，对于每个$B_i$，在计算完reward之后实例选取器的参数要更新一次，而CNN分类器的参数只有在整个epoch结束才会更新。 结果因为后两个模型是bag level的，所以在评价的时候是把一个句子当成一个bag来处理的，最终结果如下：可以看出文章的模型取得最好的performance，而且句子层次的模型相比较于bag层次的模型，在这个评测中表现更好，这也是理所当然。 附加分析分别对实例选取器和关系分类器作了分析： 分析加入实例选取器是否对performance提高有帮助（方法：在原始数据和选取出来的数据上分别训练模型）。 评价了选取器的accuracy（方法：随机选取300个句子，人工判断，最终的accuracy达到(54+177)/300=74%）。 比较了强化学习机制和贪婪机制（greedy selection）之间的差异（方法：所谓贪婪机制就是把强化学习机制改成选取CNN中likelihood最大的N个句子，当然这个N与强化学习机制选出的句子数目一致）。 验证选取器是否能区分出那些全是噪声数据的bag（方法：随机选取100个模型认为是全噪声的bag，其中有86%真的是全噪声的bag，可以证明模型把绝大多数的全噪声bag过滤了）。 思考 第一次阅读强化学习用在NLP上的文献，大致了解强化学习的基本思想，看到强化学习在NLP领域是有所作为的。 模型的State表示是由3部分组成，分别包含句子、句子集合、实体对的信息，这个State表示是否可以改变，是不是越复杂越好，包含的信息越多越好，这些应该会在下面的工作中有探索。 本篇文章设定的Action比较简单，但是这个Action确实是结合实际需要提出的，我不太明白是否Action设定更加复杂之后会不会对模型的performance有影响。 这篇工作是基于噪声数据做的，现在NLP在实际应用中，标注数据的不足、质量不高是比较大的问题，假设我们能用比较简单但是准确率不太高的方式预标注，再加入强化学习机制做模型训练，也许可以解决标注不足的问题。 可以探索更多能够加入强化学习机制的NLP任务，而不是仅限于噪声数据筛选这一项，比如本文同实验室的另一篇AAAI 2018的文章Learning Structured Representation for Text Classification via Reinforcement Learning，这篇文章也是要解决文本分类问题，但是角度又不一样了，是用Reinforcement Learning学习结构化表示来提高分类效果，而在强化学习上又分为两种：删去非关键词和切分词组，具体可以查看文章。]]></content>
      <categories>
        <category>文献库</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Deep Learning</tag>
        <tag>Paper</tag>
        <tag>Relation Classification</tag>
        <tag>Reinforcement Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Semi-supervised sequence tagging with bidirectional language models]]></title>
    <url>%2F2017%2Fpaper_0_LM%2F</url>
    <content type="text"><![CDATA[最近在看微信推送的时候，有看到一篇推荐文献，发表在ACL2017，跟我之前想做的 通过改进embedding的表示提高NLP任务的表现 方面有点联系的，大致看下来觉得挺不错的，这里在仔细阅读一遍，记录下来。这篇文章不管是在对技术上、研究思想上，还是文章写作方面都有借鉴意义。链接：Semi-supervised sequence tagging with bidirectional language models 摘要预训练的word embedding已经在基于深度学习的NLP模型里广泛应用了，但是word embedding往往只从数量比较少的数据中训练得来。文章提出了将一个双向的语言模型的context embedding引入序列标注任务中，在NER和chunking两个任务上进行了实验，发现context embedding的引入可以提高performance，并且与其他的某些transfer或者joint learning，以及某些引入附加信息的方法相比，能得到更好的performance。 模型上图是整个模型的概览，左边是一个比较基本的Bi-RNN-CRF的序列标注模型，也可以参考End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF，这个模型最开始输入的character representation也是由两部分组成，一部分是形态学特征信息，$c_k$，另一部分是预训练的word embedding，$w_k$。接着是一个正向一个反向RNN，最后接一个CRF层对标签进行优化。语言模型（Language Model）本来是用来计算一个序列 $(t_1, t_2, \cdots, t_N)$ 的可能性，即： $p(t_1, t_2, \cdots, t_N) = \prod_{k=1}^N{p(t_k|t_1, t_2, \cdots, t_{k-1})}$ 基于以上的基本假设，构建深度神经网络，最后得到一个固定长度的向量，$h_f^{LM}$。上面公式其实是得到一个正向的表示，考虑到既然可以获得正向上下文信息，作者又训练了一个反向的语言模型，也得到一个固定长度的向量，$h_b^{LM}$。主要的计算公式如下： $p(t_1, t_2, \cdots, t_N) = \prod_{k=1}^N{p(t_k|t_{k+1}, t_{k+2}, \cdots, t_N)}$ 最后两个语言模型的embedding，$h_f^{LM}$，$h_b^{LM}$，串联组成一个向量$h^{LM}$。这里的两个向量完全是独立训练得到，没有共享参数。在得到$h^{LM}$后，作者将这个向量和Bi-RNN第一层的输出串联起来当成第二层RNN的输入。 其实，作者也提出了几个方案来做$h^{LM}$和第一层输出的整合，包括1. 串联之后加一个非线性变换；2. 加入类似attention机制，来给$h^{LM}$加上权重。因为直接串联的效果已经比较好了，所以作者并没有实现。 试验作者主要在两个任务上做了评测，CoNLL 2003 NER task和CoNLL 2000 Chunking task。采用的标注方式是BIOES。 训练中的tricks Adam optimizer 用early stopping来防止overfitting，具体策略是：先固定learning rate，在每个epoch训练结束之后，都测试一下开发集上的的performance。假如得到了一个最好的performance，就把learning rate作退火处理（annealing），即降低一个数量级（比如除以10），继续训练5个epoch，再降低一个数量级，继续训练5个epoch就结束训练。我的理解是假如说连续训练5个epoch，再降低learning rate训练5个epoch还没有出现更好的模型，就停止训练。 随机种子，训练10次，取F值的平均和标准差 结果从上图可以看出，文章的模型与baseline相比，表现最好，而且加入语言模型embedding，可以显著提高系统performance。 附加分析 在模型的哪个模块加入语言模型embedding，输入层，第一层RNN的输出，第二层RNN的输出？ 第一层RNN的输出，解释是可能第二层的RNN可以把第一层RNN的任务特殊性的上下文信息和语言模型中通用的上下文信息结合起来考虑。 使用不同的语言模型是否会有影响？ 加入反向语言模型更好，比较大的语言模型的size更好，比较大的语料用来训练语言模型更好。 评测数据集大小？ 在小的数据集上，加入语言模型后，performance提高更显著。 是不是因为语言模型embedding的加入增加了参数，所以performance更好？通过两个实验：1. 不加LM，只是单纯增加第二层RNN的维度（没有提及怎么增加这个维度的？）与加入LM时的维度一致；2. 加LM，但是对维度进行压缩（同样没有提及怎么压缩这个维度的？），使得与不加LM的基础模型维度一致。结论是：单纯增加维度是没用的，压缩维度会稍微降低performance。 语言模型可以跨领域吗？ 将新闻领域训练出来的语言模型，用到科学出版物的任务上，也有performance的提升。 思考 即使是dropout这么一个参数就有好多选择要不要用，在哪些层要用，用多大，比如在CoNLL 2003 NER的基本模型中，作者就只在GRU的输入上用了25%的dropout；在CoNLL 2000 chunking的基础模型中就在embedding上，LSTM的输入，以及最后一层LSTM的输出上用了50%的dropout。 对人们比较关心的几个问题都做了附加的分析，让人心服口服，值得学习。 即使只是对基础模型的一个细小改动，增加一个类型信息（本文是语言模型的上下文信息），如果把实验做的充分，结果显著，也是可以发出来文章的。]]></content>
      <categories>
        <category>文献库</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Deep Learning</tag>
        <tag>Embedding</tag>
        <tag>Paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown 小技巧]]></title>
    <url>%2F2017%2Ftricks_in_markdown%2F</url>
    <content type="text"><![CDATA[MathJax 公式在更新后的Material 1.5.0版本的post中加入了MathJax选项，即可以选择是否启用MathJax公式渲染，具体语法可以参见 MathJax官网 。但是我在渲染公式的时候还是有点问题，记录一下配置过程。其实只要在post的head里设置mathjax: true就可以了。而且mathjax这个词是区分大小写的，千万要注意。如果需要使用需要在两边加上$。下面是常用的一些公式： 意义 格式 显示 上标 x^y $x^y$ 多上标 x^{y^z} $x^{y^z}$ 下标 x\_i $x_i$ 希腊字母 \alpha $\alpha$ 希腊字母 \beta $\beta$ 希腊字母 \Delta $\Delta$ 无穷 \infty $\infty$ 点 \cdot, \cdots $\cdot$, $\cdots$ 运算符 \times, \div, \lt, \gt, \le, \ge, \neq $\times$, $\div$, $\lt$, $\gt$, $\le$, $\ge$, $\neq$ 求和 \sum\_{i=0}^n $\sum_{i=0}^n$ 分数 \frac{x}{y} $\frac{x}{y}$ 开根号 \sqrt 3 $\sqrt 3$ 开根号 \sqrt[3] 3 $\sqrt[3] 3$ 极限 \lim\_{x \to 0} $\lim_{x \to 0}$ 分情况 f: \begin {cases} x, x&gt;0 \\\ \\\ -x, x&lt;0 \end {cases} $f: \begin {cases} x, x&gt;0 \\ \\ -x, x&lt;0 \end {cases}$ 连等式 \begin{eqnarray} \sum_{ k = 1 }^{ n } k^2 = \overbrace{ 1^2 + 2^2 + \cdots + n^2 }^{ n } = \frac{ 1 }{ 6 } n ( n + 1 ) ( 2n + 1 ) \end{eqnarray} $\begin{eqnarray} \sum_{ k = 1 }^{ n } k^2 = \overbrace{ 1^2 + 2^2 + \cdots + n^2 }^{ n } = \frac{ 1 }{ 6 } n ( n + 1 ) ( 2n + 1 ) \end{eqnarray}$ 参考：http://daniellaah.github.io/2016/Mathmatical-Formula-within-Markdown.html 链接中含有()正常的链接格式为： [link text](URL &#39;title text&#39;)，但是假如URL中含有()，比如https://en.wikipedia.org/wiki/Rectifier_(neural_networks)。如果还是用正常的链接格式，就会多出来一个括号，而且链接也是错误的，像下面这样： 例子) 然后可以采用参考式的形式：1.定义链接文字： [link text][id] 其中id可以是数字、字母、标点等的唯一标识符。 2.定义链接网址： [id]: URL "title" 其中URL必须加上http或者https，这个内容可以放在文件的任意位置。 3.省略id，直接用link text来指针链接： [link text][] [link text]: URL "title" 这个方式还有一个好处就是可以在不同的地方调用同一个链接。 当然，还学到一种超链接的表示形式 自动链接 ，即用&lt;&gt;将URL或者邮箱地址括起来，就能将URL直接转换成超链接文字，非常方便啊。 参考：http://xianbai.me/learn-md/article/syntax/links.html]]></content>
      <categories>
        <category>技术向</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Excel小技巧]]></title>
    <url>%2F2017%2Fexcel_tricks%2F</url>
    <content type="text"><![CDATA[我自己之前在做某些Excel操作时，有些技巧是百度之后才学来的，特地分享一波。 数字转文本出现“小数E+12”figure 1里D列是长数值格式，转成figure 2中的文本格式后出现“小数E+12”的格式，可以像figure 3中一样通过“数据-分列-下一步-下一步”，把“列数据格式”设置成“文本”，然后“完成”，即可转为figure 4中的样式。 参考：https://wenku.baidu.com/view/7ebc09fdf78a6529657d5338.html 日期格式转特定文本格式如下图，需要将左侧的日期格式转换成右边连写的文本格式，可以利用公式： =text(A2,"yyyymmdd") 其中yyyymmdd是需要定义的日期文本格式，也可以加入时、分、秒等信息，比如yyyymmddhhmmss就会生成“20171020141212”这样的文本，yyyymmdd hh:mm:ss就会生成“20171020 14:12:12”这样。在转成文本格式之后，可能还会在文本末尾加上后缀等等，用&amp;将后缀加在文本后就可以了，比如=text(A2,&quot;yyyymmdd&quot;)&amp;&quot;001&quot;就是在日期后加上001。 参考：http://blog.sina.com.cn/s/blog_598ae3e30100a4jw.html]]></content>
      <categories>
        <category>技术向</category>
      </categories>
      <tags>
        <tag>Excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[不被注意的Python代码规范(2)---语言规范]]></title>
    <url>%2F2017%2Fpython_code_rules_2%2F</url>
    <content type="text"><![CDATA[本篇接上一篇：不被注意的Python代码规范(1)—风格规范，主要介绍Pythn代码的语言规范。主要的参考是Google Python Style Guide。 PylintPylint是一个代码检查工具，用来发现代码bug和不符合规范的地方，默认代码风格是PEP 8。但是目前流行的IDE，如eclipse，pycharm都包含了代码检查功能。 包在代码中通过包的全名来import每个模块，比如： # 用全名引用. import sound.effects.echo # 只用模块名引用 (推荐). from sound.effects import echo 异常处理异常处理是被允许的，但是使用的时候要注意。其实之前都没注意过这方面的细节，就会用一个try…except…，上具体代码学习学习： try: # 正常操作 pass except Exception, expression: # 可以有多种异常，满足一个就执行异常代码，代码：except(Exception1[, Exception2[, ...]]) print expression # 获得expression作为错误的具体表述 else: # 如果没有异常发生 finally: # 不管有没有异常发生，都会执行 还有几个点需要注意的： 异常的识别比较敏感，会把各种基本异常都识别到，所以尽量指定异常名。 尽量减少try后面的代码操作，代码越多，异常可能也越多，而让我们catch不到我们关心的异常。 Lambda函数Lambda函数定义的是匿名函数，经常和map()、filter()函数连用。主要好处是可以减少定义那些只用到一次的函数。但是不需要用来代替某些内置函数，比如乘法之类的，又来一波实践代码： >>> l = ['foo', 'bar', 'far'] >>> map(lambda x: x.upper(), l) # 取list中所有词的大写 ['FOO', 'BAR', 'FAR'] >>> filter(lambda x: 'f' in x, l) # 筛选list中含有'f'的词 ['foo', 'far'] >>> reduce(lambda x, y: x*y, xrange(1,5)) # xrange与range的区别是xrange得到的是一个生成器，reduce函数与map函数差不多，是一个反复调用的功能。 24 装饰器本来对装饰器一无所知，之前还和同事讨论过装饰器的一些用法之类的，大致了解了。感觉它的主要目的是提取函数中与本身功能无关的一些代码，进而达到代码重用的目的。我主要是参考http://python.jobbole.com/82344/这篇博客，循序渐进地描述了把一个计时功能逐渐转成装饰器的过程，从实现最基本功能，到函数加入参数，再到装饰器加入参数，比较容易理解，很推荐。我把代码重写一遍，加深一下理解。 # coding:utf8 import time def time_count(func): s_time = time.time() func() e_time = time.time() time_c = e_time - s_time print 'elapsed time: {} s.'.format(time_c) def func(): print 'strat' time.sleep(0.5) print 'end' time_count(func) # 以上代码是实现func函数的运行时间计算，但是所有的func函数都要以time_count(func)来运行了。 # 以下是用装饰器重写 import time def time_count(func): def wrapper(): s_time = time.time() func() e_time = time.time() time_c = e_time - s_time print 'elapsed time: {} s.'.format(time_c) return wrapper @time_count def func(): print 'strat' time.sleep(0.5) print 'end' func() 至此，对于Python的代码规范大致总结了一些我个人之前忽略了的方面，也希望之后在coding的过程中进一步应用，写出漂亮的代码。]]></content>
      <categories>
        <category>技术向</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Code</tag>
        <tag>Standard</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[不被注意的Python代码规范(1)---风格规范]]></title>
    <url>%2F2017%2Fpython_code_rules_1%2F</url>
    <content type="text"><![CDATA[最近已经在找工作了，发现各个公司对代码能力要求还是比较高的，谁叫咱还是要干码农的活呢，所以决心改一改自己的代码习惯，写出漂亮的代码。虽说重要的是实现功能，但是代码看起来漂亮，也会让人更舒服吧。下面会记录一些自己以前不太注意到的地方，主要准则还是“有则改之，无则加勉”。主要分成风格规范和语言规范两类。这篇是讲风格规范。主要的参考是Google Python Style Guide。 最长行最大行长度不超过80个字符，很长的导入语句或者注释里的URL除外。如果一个文本串的长度超过了80，可以以括号的形式来隐性连接两段短文本，如下： x = ('This will build a very long long long long long ' 'long long long long long long string') 缩进用 4个空格 作为缩进符当字符数大于80了，可以有两种缩进方式选择，一种是超过80的部分，以固定缩进长度表示；第二种是以4个空格作为缩进，但是这种情况下，第一行就不能有任何内容，具体例子如下： # 固定缩进长度 foo = long_function_name(var_one, var_two, var_three, var_four) # 在字典中固定缩进长度 foo = { long_dictionary_key: value1 + value2, ... } # 4个空格缩进，第一行没有任何东西 foo = long_function_name( var_one, var_two, var_three, var_four) # 在字典中以4个空格缩进 foo = { long_dictionary_key: value1 + value2, ... } 空行高级定义（函数或者类）之间空两行，方法定义之间，以及class定义行和第一个方法定义行之间空一行。 空格根据印刷标准的要求，来使用标点周围的空格。当’=’出现在表示关键值或者默认参数值时，周围不需要加空格，如下： def complex(real, imag=0.0): return magic(r=real, i=imag) 注释对于函数、方法的注释，应该使用文档字符串，当然假如函数非常短小，简单明了，就不必要注释了。下面会有具体的例子。 文档字符串是包、模块、类或函数里的第一个语句。这些字符串可以通过对象的__doc__成员被自动提取，并且被pydoc所用。(你可以在你的模块上运行pydoc试一把, 看看它长什么样)。我们对文档字符串的惯例是使用三重双引号”””( PEP-257 )。一个文档字符串应该这样组织：首先是一行以句号，问号或惊叹号结尾的概述(或者该文档字符串单纯只有一行)。接着是一个空行。接着是文档字符串剩下的部分，它应该与文档字符串的第一行的第一个引号对齐。 具体的文档字符串应该包括： Args: 列出所有参数名字，并对参数进行描述 Returns: 描述返回值的类型和语义 Raises: 列出与接口有关的所有异常 def fetch_bigtable_rows(big_table, keys, other_silly_variable=None): """Fetches rows from a Bigtable. Retrieves rows pertaining to the given keys from the Table instance represented by big_table. Silly things may happen if other_silly_variable is not None. Args: big_table: An open Bigtable Table instance. keys: A sequence of strings representing the key of each table row to fetch. other_silly_variable: Another optional variable, that has a much longer name than the other args, and which does nothing. Returns: A dict mapping keys to the corresponding table row data fetched. Each row is represented as a tuple of strings. For example: {'Serak': ('Rigel VII', 'Preparer'), 'Zim': ('Irk', 'Invader'), 'Lrrr': ('Omicron Persei 8', 'Emperor')} If a key from the keys argument is missing from the dictionary, then that row was not found in the table. Raises: IOError: An error occurred accessing the bigtable.Table object. """ pass 对于类的注释，也应该有一个用于描述该类的文档字符串。而且如果你的类有公共属性(Attributes)，那么文档中应该有一个属性(Attributes)段。并且应该遵守和函数参数相同的格式。具体例子如下： class SampleClass(object): """Summary of class here. Longer class information.... Longer class information.... Attributes: likes_spam: A boolean indicating if we like SPAM or not. eggs: An integer count of the eggs we have laid. """ def __init__(self, likes_spam=False): """Inits SampleClass with blah.""" self.likes_spam = likes_spam self.eggs = 0 def public_method(self): """Performs operation blah.""" 字符串即使在参数都是字符串的情况下，也应该使用format或者%方法来格式化字符串。但是还是要在+和format（或者%）判断性的选择。从下面的例子中，是不是说，假如只是两个字符串组合，就直接使用+，而假如是字符串潜入到别的内容中，就最好使用format或者% x = a + b x = '%s, %s!' % (imperative, expletive) x = '{}, {}!'.format(imperative, expletive) x = 'name: %s; score: %d' % (name, n) x = 'name: {}; score: {}'.format(name, n) 在循环里不要使用+或者+=的形式来累加字符串。尽管字符串是不变的，但是这样会生成很多不必要的临时对象和结果，进而会造成运行时间平方次的增长，而不是线性增长。因此把自字符串先加到一个list里，然后在循环结束后join起来是更好的选择。 items = ['&lt;table>'] for last_name, first_name in employee_list: items.append('&lt;tr>&lt;td>%s, %s&lt;/td>&lt;/tr>' % (last_name, first_name)) items.append('&lt;/table>') employee_table = ''.join(items) 其他小规范：在同一个文件中使用统一的字符串引号；使用&quot;&quot;&quot;来表示多行字符串。 文件在使用完文件之后，要关闭文件，推荐使用with的方式打开文件，因为这样会自动关闭文件。 with open("hello.txt") as hello_file: for line in hello_file: print line 导入格式每一行只导入一个模块，模块导入顺序如下： 标准库导入 第三方库导入 程序指定导入 在同一个组中，又要按字母顺序导入，且忽略大小写 import foo from foo import bar from foo.bar import baz from foo.bar import Quux from Foob import ar 命名命名约定： 所谓”内部(Internal)”表示仅模块内可用，或者在类内是保护或私有的。 用单下划线(_)开头表示模块变量或函数是protected的(使用import * from时不会包含。 用双下划线(__)开头的实例变量或方法表示类内私有。 将相关的类和顶级函数放在同一个模块里。不像Java，没必要限制一个类一个模块。 对类名使用大写字母开头的单词(如CapWords，即Pascal风格)，但是模块名应该用小写加下划线的方式(如lower_with_under.py).。尽管已经有很多现存的模块使用类似于CapWords.py这样的命名，但现在已经不鼓励这样做，因为如果模块名碰巧和类名一致，这会让人困扰。 下面是具体例子 Type Public Internal Modules lower_with_under _lower_with_under Packages lower_with_under Classes CapWords _CapWords Exceptions CapWords Functions lower_with_under() _lower_with_under() Global/Class Constants CAPS_WITH_UNDER _CAPS_WITH_UNDER Global/Class Variables lower_with_under _lower_with_under Instance Variables lower_with_under _lower_with_under (protected) or __lower_with_under (private) Method Names lower_with_under() _lower_with_under() (protected) or __lower_with_under() (private) Function/Method Parameters lower_with_under Local Variables lower_with_under 正文到此结束，确实学到了很多代码规范，虽然写了2年多Python了，但是某些细节上的东西并没有注意太多。当然，正如参考文章最后提到的保持一致才是最终的目的，也就是与已有的代码风格保持一致，也许有些地方并不完全遵照上文里的规范，但是为了风格一致性，可以少许放弃规范，可以实现更好的代码沟通。]]></content>
      <categories>
        <category>技术向</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Code</tag>
        <tag>Standard</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Blog 折腾记]]></title>
    <url>%2F2017%2Fhexo_blog_personalize%2F</url>
    <content type="text"><![CDATA[本着不折腾不舒服的精神，对博客进行个性化，丰富功能。 修改不蒜子统计Material 主题的PV和UV统计用的也是不蒜子，但是显示的位置在分享里，太隐蔽了。于是乎，我就想把统计信息显示在footer里，然后参照网上教程，修改了layout/_partial/footer.ejs，如下： &lt;script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">&lt;/script> &lt;span id="busuanzi_container_site_pv">本站总访问量&lt;span id="busuanzi_value_site_pv">&lt;/span>次&lt;/span> 增加每日一图每日一图的功能是我在搜索Material主题的博客时，偶然发现的一个隐藏属性，具体可以参见这里，因为我发现这个作者给的API比较慢，于是就搜了一下有没有比较快的API，结果还真有，就是这个，这个API返回的是Bing的图片，图片质量很有保证，而且作者有很多参数可以设置，我选择了分辨率为800*480，这样加载速度更快，十分满意，具体修改layout/_partial/daily_pic.ejs，如下： &lt;div class="mdl-card__media mdl-color-text--grey-50" style="background-image:url(https://bing.ioliu.cn/v1/rand/?w=800&amp;h=480)"> 其中w是设置宽度，h是设置高度，具体的参数设置可以参考github页面。总之，给这个API的作者点个赞，哈哈。 增加版权信息为每篇文章增加版权信息，其实也没人会来转载我的文章，但是就是想尽可能完善一下自己的博客嘛。我参考的是这位仁兄的方法，结合了一些别的博客看到的版权声明。具体我是修改layout/_partial/post-content.ejs，如下： &lt;% if(!page.nostatement){ %> &lt;div class="article-statement"> &lt;hr> &lt;strong>本文标题: &lt;/strong>&lt;a href="&lt;%- url_for(page.path) %>">&lt;%= page.title %>&lt;/a>&lt;br> &lt;strong>原始链接: &lt;/strong>&lt;a href="&lt;%- url_for(page.path) %>" title="&lt;%= page.title %>">&lt;%= page.permalink %>&lt;/a>&lt;br> &lt;strong>发布时间: &lt;/strong>&lt;%= page.date.format("YYYY年MM月DD日-HH:mm") %>&lt;br> &lt;% if(page.updated){ %> &lt;strong>最后更新: &lt;/strong>&lt;%= page.updated.format("YYYY年MM月DD日-HH:mm") %>&lt;/p> &lt;% } %> &lt;strong>版权声明: &lt;/strong>本站文章均采用&lt;a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0协议&lt;/a>进行许可。转载请注明出处！&lt;br> &lt;/div> &lt;% } %> 这里增加了nostatement参数，主要是考虑到About页面并不需要版权信息，加了这个参数可以自由控制版权信息的添加。 增加代码高亮在增加了上面这些代码以后，发现Material主题的代码渲染不是很好看，于是去查了作者的说明文档，发现可以安装插件Hexo-Prism-Plugin，我按照github页面安装配置。 npm i -S hexo-prism-plugin prism_plugin: mode: preprocess # realtime/preprocess theme: solarizedlight line_number: true # default false 还是不能渲染，最后参照这篇博客，发现是我的theme写错了，所以不能高亮，然后我把所有的theme都试了一遍，最后选择了solarizedlight，显示效果在上面。主要theme有如下几种： default coy dark funky okaidia solarizedlight tomorrow twilight 在.md文件中插入代码块的语法如下，其中language可以包括绝大部分编程语言，具体参考官方文档 ```language 代码块 ``` About页面增加背景音乐 毋庸置疑，好的事情总会到来。而当它来晚时，也不失为一种惊喜。 ——— 《托斯卡纳艳阳下》 这个是心血来潮的想法，之前听过《Almost Lover》，主要是配合女神汤唯和廖凡演的《命中注定》电影原声MV，简直不能更唯美心碎，真的推荐这个MV，刚刚我又循环了10遍。是的，刚刚也把电影刷了一遍。于是想把 它加过来做背景音乐，然后加在了About me页面，方法其实很简单。在网易云音乐上搜索歌名，选择生成外链播放器。得到的外链代码大致如下： &lt;iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&amp;id=22423722&amp;auto=1&amp;height=66">&lt;/iframe> 然后在.md文件中插入这段代码就可以生成播放器，具体效果可以参考About me页面。 更换评论系统为gitment在升级了Material 1.5之后，发现评论系统disqus_click选项会根据网络环境自动加载，这就失去了click的意义了，然后发现1.5版本的主题开始支持其他的评论系统，比如gitment，也就是我后面要换成的评论系统。选择这个有以下几个考虑： 玩博客的人肯定用github，保证了适用性。 配置比较方便，下面我会说具体过程。 可以不用科学上网就可以访问。 下面是具体配置过程，因为我是在Material主题的基础上配置，所以有些步骤直接省掉了，其他主题的可以参看作者的说明文档，挺详细的，点个赞！ 点击链接，注册OAuth application 得到一个 client ID 和一个 client secret，这个将被用于之后的配置。 修改theme里的config文件，如下： 配置完成之后，点击登录按钮，然后会出现初始化按钮，点击之后会在上面新建的repo的issue里建立一个新的issue，然后就可以评论了。 这个作者的这个评论系统做的挺好的，支持代码高亮什么的，可以再研究研究。 我会继续折腾，本页面也将持续更新。]]></content>
      <categories>
        <category>分享境</category>
      </categories>
      <tags>
        <tag>Blog</tag>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Introduction to RNNs]]></title>
    <url>%2F2017%2Fintroduction_to_rnn%2F</url>
    <content type="text"><![CDATA[最近在找关于deep learning的基础知识看，找到一个比较推荐的英文博客介绍，为了加深一下映象，决定翻译一遍。采取中英文对照的形式。 What are RNNs?The idea behind RNNs is to make use of sequential information. In a traditional neural network we assume that all inputs (and outputs) are independent of each other. But for many tasks that’s a very bad idea. If you want to predict the next word in a sentence you better know which words came before it. RNNs are called recurrent because they perform the same task for every element of a sequence, with the output being depended on the previous computations. Another way to think about RNNs is that they have a “memory” which captures information about what has been calculated so far. In theory RNNs can make use of information in arbitrarily long sequences, but in practice they are limited to looking back only a few steps (more on this later). Here is what a typical RNN looks like: Recurrent Neural Networks (RNNs)背后的思想是充分利用序列信息。在传统的神经网络中，我们是假设所有的输入（或者输出）之间是相互独立的。但是在一些任务中，这个假设是非常不合理的。比如，如果你想预测某个句子中的下一个词，那么最好能知道这个词的前一个词是什么。RNN之所以叫循环的（recurrent），是因为它对于序列中的每一个元素执行相同的操作，而且输出是依赖于前一步的计算。也可以从另一种角度来理解RNN，就是它拥有记忆前面所有计算得到的信息的能力。理论上，RNN可以利用任意长度序列中的信息，但是在实践中只能回溯到有限的几步。下面是一个典型的RNN图示：Source: Nature The above diagram shows a RNN being unrolled (or unfolded) into a full network. By unrolling we simply mean that we write out the network for the complete sequence. For example, if the sequence we care about is a sentence of 5 words, the network would be unrolled into a 5-layer neural network, one layer for each word. The formulas that govern the computation happening in a RNN are as follows: 上面的图展示了一个RNN展开成全网络的情况。通过展开，我们简单的把整个序列表示成网络形式。举个例子，如果序列的长度是5，那么整个网络会被展开成5层，每一层对应一个词。图中的参数以及一些计算公式如下： $x_t$ is the input at time step $t$. For example, $x_1$ could be a one-hot vector corresponding to the second word of a sentence.$s_t$ is the hidden state at time step $t$. It’s the “memory” of the network. $s_t$ is calculated based on the previous hidden state and the input at the current step: $s_t=f(Ux_t + Ws_{t-1})$. The function $f$ usually is a nonlinearity such as tanh or ReLU. $s_{-1}$, which is required to calculate the first hidden state, is typically initialized to all zeroes.$o_t$ is the output at step $t$. For example, if we wanted to predict the next word in a sentence it would be a vector of probabilities across our vocabulary. $o_t = \mathrm{softmax}(Vs_t)$.There are a few things to note here: $x_t$ 是在时间步骤 $t$ 的输入。比如： $x_1$ 可能就是一个句子中的第二个词的one-hot向量。$s_t$ 是时间步骤 $t$ 的隐含状态，也就是这个网络的“记忆”。$s_t$ 是基于前一个隐含状态和当前步骤的输入计算得到，计算公式：$s_t=f(Ux_t + Ws_{t-1})$。公式 $f$ 一般是非线性函数，比如：tanh 或者 ReLU。$s_{-1}$ 是在计算第一个隐含状态是需要的值，会被初始化为0。$o_t$ 是步骤$t$的输出。比如，如果我们想预测一个句子中的下一个词，那么 $o_t$ 可能就是一个基于整个词汇集的概率向量。公式：$o_t = \mathrm{softmax}(Vs_t)$下面是需要注意的几点： You can think of the hidden state $s_t$ as the memory of the network. $s_t$ captures information about what happened in all the previous time steps. The output at step $o_t$ is calculated solely based on the memory at time $t$. As briefly mentioned above, it’s a bit more complicated in practice because $s_t$ typically can’t capture information from too many time steps ago. 我们可以把隐含状态 $s_t$ 看成是网络的记忆。$s_t$ 能获取前面所有时间步骤中的所有信息。时间步骤 $t$ 的输出 $o_t$ 只基于时间 $t$ 的记忆计算得到。就像上面提到的，实际操作中会更加复杂，因为 $s_t$ 并不能获得太多时间步骤前的信息。 Unlike a traditional deep neural network, which uses different parameters at each layer, a RNN shares the same parameters ($U$, $V$, $W$ above) across all steps. This reflects the fact that we are performing the same task at each step, just with different inputs. This greatly reduces the total number of parameters we need to learn. 不像传统的深度神经网络那样在每层都采用不同的参数，RNN在所有步骤中都是用的相同的参数集（上图中的$U$, $V$, $W$）。这反映的事实就是我们在每步都进行了相同的操作，只是每步的输入不一样。这使得要学习的参数数量大大降低。 The above diagram has outputs at each time step, but depending on the task this may not be necessary. For example, when predicting the sentiment of a sentence we may only care about the final output, not the sentiment after each word. Similarly, we may not need inputs at each time step. The main feature of an RNN is its hidden state, which captures some information about a sequence. 上图中每个步骤都有输出，但是基于不同的任务并不是每个步骤都需要输出的。比如在做句子情感分析的时候，我们只关心最终的输出，而不是每个词的情感分析结果。类似的，我们也不需要每个步骤都有输入。RNN的一个主要特征就是隐含状态，这样能够获得整个序列的某些信息。 What can RNNs do?RNNs have shown great success in many NLP tasks. At this point I should mention that the most commonly used type of RNNs are LSTMs, which are much better at capturing long-term dependencies than vanilla RNNs are. But don’t worry, LSTMs are essentially the same thing as the RNN we will develop in this tutorial, they just have a different way of computing the hidden state. We’ll cover LSTMs in more detail in a later post. Here are some example applications of RNNs in NLP (by non means an exhaustive list). RNN已经在很多的NLP任务中取得了成功。最常用的RNN是LSTM，它比获取长依赖信息上比vanilla RNNs表现更好。但是，LSTM和我们想建立的RNN模型实际上是一样的，只是用了不同的方法来计算隐含状态。我们会在后面讨论LSTM的更多细节。下面是RNN在NLP领域上应用的例子。 Language Modeling and Generating TextGiven a sequence of words we want to predict the probability of each word given the previous words. Language Models allow us to measure how likely a sentence is, which is an important input for Machine Translation (since high-probability sentences are typically correct). A side-effect of being able to predict the next word is that we get a generative model, which allows us to generate new text by sampling from the output probabilities. And depending on what our training data is we can generate all kinds of stuff. In Language Modeling our input is typically a sequence of words (encoded as one-hot vectors for example), and our output is the sequence of predicted words. When training the network we set $o_t = x_{t+1}$ since we want the output at step $t$ to be the actual next word. 语言建模和文本生成 给定一个词序列，我们想预测每个词出现在它前一个词后面的概率。语言模型可以让我们能够估算一个句子有多像一个正常的句子（或者说符合语法、符合自然语言的句子），这在机器翻译中是十分重要的（因为更高的可能性指正着更高的准确率）。预测下一个词的功能带来的另一个功能就是我们也可以得到一个生成模型，使得我们可以根据输出概率来生成新的文本。而且，根据我们的训练数据的不同，可以生成各种各样的文本素材。在语言模型中，我们的输入通常是词序列（比如：编码成one-hot的向量），输出就是预测词的序列。在训练整个网络的时候，我们设置 $o_t = x_{t+1}$ ，因为我们想得到步骤 $t$ 的输出就是下一个词。 Research papers about Language Modeling and Generating Text: 关于语言建模和文本生成的文章： Recurrent neural network based language model Extensions of Recurrent neural network based language model Generating Text with Recurrent Neural Networks Machine TranslationMachine Translation is similar to language modeling in that our input is a sequence of words in our source language (e.g. German). We want to output a sequence of words in our target language (e.g. English). A key difference is that our output only starts after we have seen the complete input, because the first word of our translated sentences may require information captured from the complete input sequence. 机器翻译 和语言建模比较类似。只是在机器翻译中，我们的输入是源语言（比如：德语）的词序列，而我们想得到的输出是目标语言（比如：英语）的词序列。关键的不同在于我们的输出是在看了所有的输入之后进行，因为翻译的句子的第一个词有可能需要整个输入序列的信息。 Source: http://cs224d.stanford.edu/lectures/CS224d-Lecture8.pdf Research papers about Machine Translation: 关于机器翻译的文章： A Recursive Recurrent Neural Network for Statistical Machine Translation Sequence to Sequence Learning with Neural Networks Joint Language and Translation Modeling with Recurrent Neural Networks Speech RecognitionGiven an input sequence of acoustic signals from a sound wave, we can predict a sequence of phonetic segments together with their probabilities. 语音识别 给定从声波中得到的声形信号序列，预测语音片段序列，以及概率。 Research papers about Speech Recognition: 关于语音识别的文章： Towards End-to-End Speech Recognition with Recurrent Neural Networks Generating Image DescriptionsTogether with convolutional Neural Networks, RNNs have been used as part of a model to generate descriptions for unlabeled images. It’s quite amazing how well this seems to work. The combined model even aligns the generated words with features found in the images. 生成图片描述 结合卷积神经网络，RNN开始被作为未标注图片的描述生成模型的一部分，而且取得令人惊喜的结果。这个联合模型甚至可以把生成的文字和找到的图片特征整合起来。 Source: http://cs.stanford.edu/people/karpathy/deepimagesent/ Training RNNsTraining a RNN is similar to training a traditional Neural Network. We also use the backpropagation algorithm, but with a little twist. Because the parameters are shared by all time steps in the network, the gradient at each output depends not only on the calculations of the current time step, but also the previous time steps. For example, in order to calculate the gradient at t=4 we would need to backpropagate 3 steps and sum up the gradients. This is called Backpropagation Through Time (BPTT). If this doesn’t make a whole lot of sense yet, don’t worry, we’ll have a whole post on the gory details. For now, just be aware of the fact that vanilla RNNs trained with BPTT have difficulties learning long-term dependencies (e.g. dependencies between steps that are far apart) due to what is called the vanishing/exploding gradient problem. There exists some machinery to deal with these problems, and certain types of RNNs (like LSTMs) were specifically designed to get around them. 训练RNN模型与训练传统神经网络模型差不多，都是使用反向传播算法，但是会有点曲折。因为参数是共享的，所以每个输出的梯度不仅取决于当前步骤的计算，也取决于前面步骤的。比如，为了计算t=4这个时刻的梯度，需要反向传播3步，求梯度之和，这也叫做“Backpropagation Through Time (BPTT)”。如果上面的描述还令你有点困惑也不用担心，之后会有一篇详细的说明。现在只需要清楚，因为存在梯度消失和梯度爆炸的问题，用BPTT训练的vanilla RNNs模型在学习长距离依赖（比如：相距很远的步骤间的依赖）时是有困难的。当然，现在也有一些机制来解决这些问题，也有像LSTM这样类型的RNN模型专门来避开这个问题的。 RNN ExtensionsOver the years researchers have developed more sophisticated types of RNNs to deal with some of the shortcomings of the vanilla RNN model. We will cover them in more detail in a later post, but I want this section to serve as a brief overview so that you are familiar with the taxonomy of models. 这么多年来，研究人员已经开发了更多复杂的RNN来解决vanilla RNN中存在的不足。我们会在后续的文章中详细介绍，下面只是简单的介绍一下，让大家有个印象。 Bidirectional RNNBidirectional RNNs are based on the idea that the output at time t may not only depend on the previous elements in the sequence, but also future elements. For example, to predict a missing word in a sequence you want to look at both the left and the right context. Bidirectional RNNs are quite simple. They are just two RNNs stacked on top of each other. The output is then computed based on the hidden state of both RNNs. 双向RNNs基于的主要思想是t时刻的输出不仅取决于序列中前面的要素，也要考虑后面的要素。举个例子，在预测句子中的缺失词时，就要同时考虑上下文。双向RNN也比较简单，只是两层RNN相互堆叠，然后输出基于两个RNN的隐含层计算得到。 Deep Bidirectional RNNDeep (Bidirectional) RNNs are similar to Bidirectional RNNs, only that we now have multiple layers per time step. In practice this gives us a higher learning capacity (but we also need a lot of training data). 深度（双向）RNNs和双向RNNs类似，只是每个时间步骤有多层。实践中会有更高的学习能力，同时也需要更多的训练数据。 LSTM networksLSTM networks are quite popular these days and we briefly talked about them above. LSTMs don’t have a fundamentally different architecture from RNNs, but they use a different function to compute the hidden state. The memory in LSTMs are called cells and you can think of them as black boxes that take as input the previous state $h_{t-1}$ and current input $x_t$. Internally these cells decide what to keep in (and what to erase from) memory. They then combine the previous state, the current memory, and the input. It turns out that these types of units are very efficient at capturing long-term dependencies. LSTMs can be quite confusing in the beginning but if you’re interested in learning more this post has an excellent explanation. LSTM网络现在应用十分广泛，上面也简单提到过。LSTM从根本上来说和RNN并没有不同，只是用了不同的函数来获得隐含状态。LSTM中的记忆单元叫做cell，可以把它想象成一个黑盒，这个黑盒的输入是前面的状态$h_{t-1}$和当前的输入$x_t$。在cell的内部决定哪些信息要被保留，哪些信息要被丢弃。然后把前面的状态、当前的记忆以及输入结合起来。实践证明，这种单元在获取长距离的依赖时十分有效。刚开始，你可能会觉得LSTM十分令人困惑，但是如果你有兴趣了解更多，后续的文章会有很好的解释说明。 ConclusionSo far so good. I hope you’ve gotten a basic understanding of what RNNs are and what they can do. In the next post we’ll implement a first version of our language model RNN using Python and Theano. Please leave questions in the comments! 到此为止，我相信你们对RNN是什么，以及RNN能做什么有个大致的了解了。下一篇文章我们将利用Python和Theano完成第一个版本的RNN语言模型。欢迎提问！]]></content>
      <categories>
        <category>技术向</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>RNN</tag>
        <tag>Introduction</tag>
        <tag>Translation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在线PPT演示工具---nodePPT 简介]]></title>
    <url>%2F2017%2Fuse_nodeppt%2F</url>
    <content type="text"><![CDATA[简介 这可能是迄今为止最好的网页版演示库 正如作者所说，nodePPT是基于node.js开发的，用Markdown语法来编写的在线PPT工具，参见Github nodePPT。下面是官方demo，当然也是用nodePPT实现。 常用命令安装npm install -g nodeppt如果出现被qiang的情况可以用下面这个命令：npm install -g nodeppt --registry=http://r.cnpmjs.org 创建支持markdown语法快速创建网页幻灯片。nodeppt create ppt-name 启动# 获取帮助 nodeppt start -h # 绑定端口 nodeppt start -p &lt;port> nodeppt start -p 8090 -d path/for/ppts # 绑定host，默认绑定0.0.0.0 nodeppt start -p 8080 -d path/for/ppts -H 127.0.0.1 启用socket控制使用socket通信（按Q键显示/关闭二维码，手机扫描，即可控制）socket须知：1、注意手机和pc要可以相互访问，2、防火墙，3、ip 方法1 使用url参数http://127.0.0.1:8080/md/demo.md?controller=socket在页面按键【Q】显示控制url的二维码和控制链接（需要隐身窗口打开），手机上可以使用左右touch滑动和摇一摇切换下一页 方法2 使用start命令行nodeppt start -c socket在页面按键【Q】显示控制url的二维码和控制链接（需要隐身窗口打开），手机上可以使用左右touch滑动和摇一摇切换下一页 打印&amp;导出PPT使用url?print=1访问页面，然后选择chrome的系统打印即可。 # 获取generate帮助 nodeppt generate -h # 使用generate命令 nodeppt generate filepath # 导出全部，包括nodeppt的js、img和css文件夹 # 默认导出在publish文件夹 nodeppt generate ./ppts/demo.md -a # 指定导出文件夹 nodeppt generate ./ppts/demo.md output/path -a 这里可以生成.html文件，但是必须是-a模式，不然单独打开时没有PPT的效果。 语法总体来说，nodePPT用的是Markdown，在此基础上做了一些扩展。 基本配置title: 演讲题目 speaker: 演讲者 url: 可以设置链接 transition: 转场效果，例如：zoomin/cards/slide，其他的可以参见github页面 files: 引入的js和css文件，多个以半角逗号隔开 theme: 皮肤样式 highlightStyle: 代码高亮样式，默认monokai_sublime usemathjax: yes 启用MathJax渲染公式 单页配置语法通过[slide]作为PPT分隔 背景[slide style=&quot;background-image:url(&#39;/img/bg1.png&#39;)&quot;] 单页动画主要有以下模式：kontext，vkontext，circle，earthquake，cards，glue，stick，move，newspaper，slide，slide2，slide3，horizontal3d，horizontal，vertical3d，zoomin，zoomout，pulse等 [slide data-transition=&quot;vertical3d&quot;] 转场回调[slide data-on-leave="outcallback" data-on-enter="incallback"] ## 当进入此页，就执行incallback函数 ## 当离开此页面，就执行outcallback函数 页面内配置语法单条动画使用方法：列表第一条加上{:&amp;.动画类型}（注意空格），下面是示例代码，其他动画类型包括moveIn，fadeIn，bounceIn，rollIn，zoomIn： * 上下左右方向键翻页 * 列表支持渐显动画 {:&amp;.moveIn} * 支持多级列表 * 这个动画是moveIn 页面内上下布局[slide] ## 主页面样式 ### ----是上下分界线 ---- nodeppt是基于nodejs写的支持 **Markdown!** 语法的网页PPT nodeppt：https://github.com/ksky521/nodeppt 页面内表格### 市面上主要的css预处理器：less\sass\stylus --- |less| sass | stylus :-------|:------:|-------:|-------- 环境 |js/nodejs | Ruby | nodejs 扩展名 | .less | .sass/.scss | .styl 特点 | 老牌，用户多，支持js解析 | 功能全，有成型框架，发展快 | 语法多样，小众 案例/框架 | [Bootstrap](http://getbootstrap.com/) | [compass](http://compass-style.org) [bourbon](http://bourbon.io) | 转场效果magic标签，转场效果有zoomin/zoomout,move,circle,earthquake,newspaper,cover-diamond,horizontal3d/horizontal,vertical3d,cover-circle等 [slide] [magic data-transition=&quot;效果&quot;] form code ===== #4个以上的=作为转换前后页面的分隔符 new code [/magic] 其他属性 字体分类 &lt;span class=&quot;text-分类&quot;&gt;text&lt;/span&gt; 字体颜色 &lt;span class=&quot;颜色&quot;&gt;text&lt;/span&gt; label分类 &lt;span class=&quot;label label-分类&quot;&gt;text&lt;/span&gt; href &lt;a href=&quot;&quot;&gt;text&lt;/a&gt; mark &lt;mark&gt;text&lt;/mark&gt; 左、右对齐 {:&amp;.pull-right} 和 {:&amp;.pull-right} 引用块 &gt; nodeppt可能是迄今为止最好用的web presentation &lt;small&gt;三水清&lt;/small&gt; {:&amp;.pull-right} 代码高亮 &lt;code class=&quot;javascript&quot;&gt;code&lt;/code&gt; 这里必须在配置中设置highlightStyle为特定风格，比如monokai_sublime 特色功能iframe 效果 可以嵌入网页等，在做demo的时候会相当有用 &lt;iframe data-src=&quot;http://www.baidu.com&quot; src=&quot;about:blank;&quot;&gt;&lt;/iframe&gt; 快速翻页 输入页码，然后enter 使用O键，开启纵览模式，然后翻页 动效样式强调 官方文档说粗体和斜体，都可以有动效，但是我的实践中只有斜体有效，之后可能还要再研究一下 按下【H】键查看效果 支持zoom.js 增加了zoom.js的支持，在演示过程中使用alt+click，则点击的地方就开始放大，再次alt+click则回复原状 使用note笔记 代码 [note]这是note[/note] 按下键盘【N】键显示note 如果使用多窗口控制，note会在控制窗口中出现，这个功能点个赞！ 使用画笔 按下键盘【P】键：按下鼠标左键，在此处乱花下看看效果。 按下键盘【B/Y/R/G/M】：更换颜色 按下【1~4】：更换粗细 按下键盘【C】键：清空画板 PPT宽度变化 按下键盘【W】键，切换到更宽的页面看效果，第二次按键返回]]></content>
      <categories>
        <category>分享境</category>
      </categories>
      <tags>
        <tag>nodePPT</tag>
        <tag>node</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[参加CCKS2017 Task_2有感]]></title>
    <url>%2F2017%2Fccks2017%2F</url>
    <content type="text"><![CDATA[总结一下参加CCKS2017 Task_2的感想和经验 前言&amp;简介全国知识图谱与语义计算大会（CCKS, China Conference on Knowledge Graph and Semantic Computing），具体介绍可以参见官网。本次会议共有两个Task： Task_1: 问题命名实体识别和链接（QEDL, Question Entity Discovery and Linking） Task_2: 电子病历命名实体识别（CNER, Clinical Named Entity Recognition） 我们参加的是Task_2，与我们现在在做的医学NLP比较相关，我之前也做过一些实体抽取的工作，不过大多是基于规则的方法。 过程竞赛任务应该是5月份放出来的，最开始给我们的示例数据是包含嵌套实体的例子，由于竞赛组织者的标注比较随意，标注存在很多不一致的地方，但我们还是做了数据转换的工作，首先转换成MAE（一个标注工具）能处理的格式，企图对数据进行标注修正。然后把数据转换成CONLL格式，用于后面的训练和测试。但是，经过多次反馈协商，最后组织者放出来的数据都是非嵌套实体的，一致性也比之前的示例数据要好，同时组织者还给出了较多的未标注数据。 Model这次用的model是比较传统的深度学习模型，Bidirctional RNN-CRF，如下图所示。这个模型在很多的NLP任务上都取得了很好的表现，当然我也做了一些改进。因为之前从来没有接触过深度学习，更没有实现过深度学习模型，我参考了github上的一个中文NER的项目，这个项目也是用的前面所说的基础模型，在人民日报的数据集上取得了比较好的performance，作者用的是Tensorflow实现。因为作者的代码写得比较清楚，我把所有代码通读了几遍，理解了基本逻辑，数据输入格式，特征加入等，然后基于我们的数据实现了一下，发现效果还可以。在实现了基本模型之后，我就开始了调参，还好公司的电脑比较给力，模型跑得相对较快，选出了最优参数。在此期间，我也读了一些基于深度学习的中文NLP的工作，想借鉴一些方法，当然有些模型比较复杂，实现这些模型也超出了我的能力。最终实现了一下此论文中的character representation的方法，然后加入了分词，词性，字典等特征，最后对分模型和总模型的结果做了一个整合得到最终结果。在本次竞赛中拿到第4名的成绩，参见官网。 Future Work 探索新的模型，比如不同的character representation方法。 充分利用未标注数据，现在未标注数据只用在了word embedding的训练中，可以探索一些semi-supervised和active learning的方法来充分利用未标注数据。 把这个模型用在已有的或者新的任务上，改变大量依赖规则的现状。 感想&amp;后记这次是我自己第二次参加NLP方面的竞赛，去年的CEGS N-GRID Shared-Tasks作为参与者，今年作为主要完成者，差别还是挺大的。有几点感想： 首先，在参加这种竞赛的时候，还是要做好分工合作，这方面我觉得自己还是做的不好，绝大部分的工作都是自己完成，从最开始的数据转换，方法调研，代码实现，参数调校，论文撰写。当然其中也得到了他人的帮助，在与公司同事的交流讨论中，也产生了许多灵感，在此也十分感谢他们。希望以后自己在这方面可以做的更好吧。 其次，就是上面也说过的一定要多读相关文献，有个积累，然后多交流，多讨论，会有意想不到的收获。 最后就是代码功力，要有用代码实现方法的能力。 已经把竞赛评测论文提交了，希望这次可以被接收吧，文章经过小修，已经被接收，还被邀请现场做Presentation，班门弄斧啊，有点紧张。 最后附上我的总结，里面有对方法和数据更详细的介绍。]]></content>
      <categories>
        <category>分享境</category>
      </categories>
      <tags>
        <tag>CCKS</tag>
        <tag>Knowledge Graph</tag>
        <tag>NLP</tag>
        <tag>Deep Learning</tag>
        <tag>NER</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[参加CCKS 2017]]></title>
    <url>%2F2017%2Fccks2017_chengdu%2F</url>
    <content type="text"><![CDATA[总结一下这次去成都参加CCKS2017的感想和经验 接上一篇参加CCKS2017 Task_2有感，评测论文被接收后，去成都参加了CCKS大会，了解了一下知识图谱相关知识，见识了各位知识图谱领域的大牛，也看到了知识图谱现阶段的应用。当然，最重要的还是去看看同为评测任务的其他组选手的方法，为进一步工作找到方向。这也是第一次做大会报告，比较紧张。下面是我的总结PPT。]]></content>
      <categories>
        <category>分享境</category>
      </categories>
      <tags>
        <tag>CCKS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Google Doodles下载]]></title>
    <url>%2F2017%2Fgoogle-doodles%2F</url>
    <content type="text"><![CDATA[最近又看到Google的搜索界面的Logo变了，有些Logo还是挺漂亮的，作为一个收集癖，想把所有的Google Logo下载过来，分享一下过程，以及最终收集到的Logo合集。 简介 涂鸦是在Google徽标的基础上，为庆祝节日、纪念日和缅怀著名艺术家、先驱者和科学家而设计创作的作品，涂鸦作品大都幽默风趣，能给用户带来一份惊喜，有些时候纯粹是即兴之作。 Google Doodles，也就是各个节日或者纪念日，Google搜索界面的涂鸦。比如下面这种：我找到了一下所有涂鸦的合集 – 链接，但是不能批量下载，然后想找一下是不是有人收集了所有的涂鸦或者有脚本可以实现爬取，很遗憾，没有！然后想着自己写个爬虫爬一下吧，发现太久不写爬虫已经忘得差不多了。最后找到一个chrome插件—小乐图客，这个插件可以实现爬取网站页面上的所有图片，这样刚好可以实现我的需求。其实我还找了一些别的这类插件，缺点主要有：收费（Bulk Image Downloader），gif有时候下载为静态（Fatkun图片批量下载），下载中断且不能修改图片名称（ImageSpark - Ultimate 图片 Downloader）。所以，最终选定小乐图客，下载速度比较快，而且可以自定义图片名字为网页上对应图片的”Title”，点个赞。 下载过程首先打开前面提到的Google涂鸦合集的网站，如下所示： 在上面的页面，点击右上角的小乐图客插件按钮，会出现如下界面： 在这个界面可以如下图所示的，定义重命名规则，定义图片保存路径，还可以预览重命名之后的图片名。最后点击存图按钮，感受“刷刷刷”下载的快感吧！ 链接分享然而，以上操作都是在qiang外面完成的，qiang内还是没有办法实现，所以我把下载的合集分享出来。下面链接里是2010-2017.9.4的所有Google Doodles，我会持续更新。我不知道是不是侵权，最好不要商用，侵权立删！度盘：https://pan.baidu.com/s/1qYqgnbY，密码：5ct6]]></content>
      <categories>
        <category>分享境</category>
      </categories>
      <tags>
        <tag>Google Doodles</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP Groups]]></title>
    <url>%2F2017%2Fnlp-groups%2F</url>
    <content type="text"><![CDATA[最近在看CCKS2017评测任务的成绩时，想查一下国内NLP方面比较厉害的课题组或者实验室，记录一下。 学术界Fudan Natural Language Processing Group 主页：http://nlp.fudan.edu.cn/ Leader：黄萱菁、邱锡鹏 代表作：中文自然语言处理工具包 FudanNLP 哈工大 社会计算与信息检索研究中心(HIT-SCIR) 主页：http://ir.hit.edu.cn/ (讲真，这个主页看着有点老了…) Leader：刘挺、秦兵 哈工大的NLP在国内应该是数一数二的，整个组也比较大，分为社会预测组(SP)、文本挖掘(TM)、问答系统组(QA)、语言分析(LA)、聊天机器人(CR)、阅读理解(RC)、作文生成(TG)、情感分析(SA组)，与工业界合作也很多（科大讯飞、微软、腾讯等）。 清华大学 自然语言处理与社会人文计算实验室 主页：http://nlp.csai.tsinghua.edu.cn/new/ Leader：孙茂松、刘洋、刘知远 北京大学 计算机科学技术研究所 语言计算与互联网挖掘研究室 主页：http://www.icst.pku.edu.cn/lcwm/index.php?title=%E9%A6%96%E9%A1%B5 Leader：万小军 中科院 计算技术研究所 自然语言处理研究组 主页：http://nlp.ict.ac.cn/2017/index_zh.php Leader：刘群 中科院 自动化研究所 模式识别国家重点实验室 自然语言处理研究小组 主页：http://www.nlpr.ia.ac.cn/cip/introduction.htm Leader：宗成庆、赵军 东北大学 自然语言处理实验室 主页：http://www.nlplab.com/ Leader：朱靖波 代表作：小牛翻译、中文句法语义分析系统 NiuParser 苏州大学 自然语言处理实验室 主页：http://nlp.suda.edu.cn/member.html Leader：周国栋、朱巧明 工业界华为 NOAH’S ARK LAB 主页：http://www.noahlab.com.hk/ Leader：李航(2017.9.15更新，李航从华为离职，即将加入今日头条) 微软亚洲研究院 自然语言计算组 主页：https://www.microsoft.com/en-us/research/group/natural-language-computing/ Leader：周明 腾讯 人工智能实验室 主页：http://ai.tencent.com/ailab/index.html Leader：张潼 今日头条 人工智能实验室 主页：http://lab.toutiao.com/ Leader：李磊 其他 其实现在很多公司都开始在AI, NLP方向发力，比如：百度、搜狗，还有像科大讯飞这样在语音识别等领域深耕多年的公司，我相信未来还会涌现出大量创业公司，在人工智能的大潮中分一杯羹。 初步统计，如有遗漏请补充（排名不分先后） 参考链接：https://www.zhihu.com/question/24366306http://blog.csdn.net/wangxinginnlp/article/details/44890553]]></content>
      <categories>
        <category>技术向</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo 主题推荐]]></title>
    <url>%2F2017%2Fhexo_theme%2F</url>
    <content type="text"><![CDATA[最近已经很多次翻来覆去找比较中意，或者说想借鉴一下的Hexo主题，但是实在没有特别给力的，基本就那么几款，总结一下。 Material首先，当然是自己博客的主题，这个主题是Material风格的。优点：比较简洁，也可以通过修改图片来使得风格更加小清新；功能全面，基本都涉及了。不足：导航栏隐藏的比较深，每次都要点一下才看得到，我觉得主页可以直接显示导航栏。链接：Demo NexT这个主题可能是我查的所有主题里被用的最多的了。优点：非常简洁，动画过渡比较细腻，功能比较全面。不足：没有我想要的相册，被用的太多，审美疲劳。链接：Demo Yilia这个主题是我最开始想要从Jekyll转到Hexo来的原因，因为他有我想要的相册。优点：简洁大方，相册很美。不足：相册是和ins联动的，实现比较复杂；有些功能缺失；布局比较挤。链接：Demo Hipaper这个主题看起来就比较简洁，小清新，比较符合我的审美吧。不足：我不喜欢他的搜索界面。链接：Demo PPOffice这个主题是把图片展示和文字结合得比较好，给人一种紧凑的感觉。优点：有一个recent的侧边栏，个人比较喜欢。不足：返回顶部的按钮在最下方，有点不习惯。链接：Demo 以上纯属个人观点，非喜勿喷。]]></content>
      <categories>
        <category>分享境</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Theme</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[复旦-港中大 大数据青年学者论坛]]></title>
    <url>%2F2017%2Ffudan_cuhk%2F</url>
    <content type="text"><![CDATA[主题：深度学习、自然语言处理、知识图谱主办：复旦大学大数据学院和香港中文大学系统工程与工程管理系 特邀报告Deep Learning for Text Understanding and Machine Comprehension这个报告的主讲人是邱锡鹏教授，FudanNLP的作者，首先介绍了NLP相关的一些基本概念，然后聚焦他本人的研究方向—机器阅读与文本推理，讲了一些基本概念、研究进展以及现存的问题等等，主要是一个热场兼承接以下各个session的作用。 Session 1Better Context-to-Sequence Frameworks and Their Applications这个报告主要分以下4个部分： Teacher forcing 讲者主要介绍了几种常见的teacher forcing方法，以及最新的进展 Adversarial Training Tricks 讲者介绍了摘要生成领域内，几个提高performance的小技巧：Copy Mechanism, Coverage or Diversity, Dual or Reconstruction and CNN based Seq2Seq Application 讲者介绍了自己将以上方法和技巧运用在研究中的一些情况。 Neural Representation Learning in NLP这个报告主要介绍了NLP里的Representation问题。从word, phrase, sentence, sentence pair 4个层次分别介绍。在word层次讲到了针对不同task可能需要特化embedding的学习，然后有提及x-word2vec，也就是引进外部知识（如：topic等）进行word2vec的训练。接下来是phrase和sentence层次。 这里还有一个亮点，就是台下有人提问说如今NLP领域大家用的方法越来越复杂，但是这些方法的语义可解释性并不强，导致某些人怀疑说NLP是否已经偏离了语言学的初衷？ 讲者的回答说很多事物都会经历一个“效果提升–质疑–解释”的过程，比如上文提到的word2vec，可解释性已经慢慢显现，所以我们有理由相信现在的方法在将来也会得到解释。我觉得这个观点是很正确的，正如现阶段的临床NLP，有些任务可能Deep Learning能够完成得很好，但是并不能用医学常识来解释，或许不久的将来，DL在临床NLP上也会得到医学解释。 Jointly Learning Word Embeddings and Latent Topics这个报告有点以上一个报告为基础，更加深入的介绍了Word Embedding和Topic Model之间的相互应用和相互提高，而且这种提高是反复的、持续的。主要思想是不同Topic中同一个词的意思可能不一样，那么embedding当然就不一样，这样根据Topic得到Embedding会更加准确；然后在文本分类，特别是短文本分类中，由于存在稀疏性较大的问题，引入embedding可以通过embedding相似来降低稀疏性，从而提高performance。 Session 2Microblog Summarization Using Conversation Structures这个报告主要做的是微博的总结（摘要式，而非生成式），用到了对话结构，提出了leader和follower的概念，其实就是找出影响比较大的几条微博，作为整个对话的总结。 Who Will Come: Identify Target Users in LocationBased Services Using Hybrid Ranking and Embedding Method这个报告讲的是根据地理签到信息来为商家进行用户推荐，主要难点是features的异质性和稀疏性问题。 Composite Task-completion Dialogue Policy Learningvia Hierarchical Reinforcement Learning这个报告主要是讲的对话系统（如chatbot）。用Hierarchical的方式来提高对话系统的训练效率，以及最终效果。 Attention-based Recurrent Generator withGaussian Tolerance for Statistical Parametric Speech Synthesis这个报告主要讲的是文字到语音生成。 Session 3Key-phrase extraction using knowledge graphs这个报告本来是我最关注的一个，因为我自己现阶段的主要工作集中在实体抽取方面，想通过结合不同的知识来提高抽取效果。但是这个其实主要是关键词抽取，以实体识别作为基础，先选出关键词候选，再通过运用知识图谱选出真正的关键词。但是演示中出了点bug，中间一段的PPT没有播放，搞的云里雾里。 Improving Quality of Knowledge Bases这个报告主要是讲的大型知识图谱的优化，主要有三个方面：补充、修正、更新。首先，补充主要是针对某些关系的缺失，比如is_a关系，可以通过is_a关系的推理来补充知识图谱的内容，这里讲者也说了许多方法来提高补充的准确性，比如环状关系、概念性的entity不可能is_a具体的entity等等，这里其实也涉及到了修正的问题，像前面说的提高准确度的方法，也可以用来修正知识图谱内的知识。更新主要是针对知识过期的问题，我觉得这个比较针对应用，因为应用内需要知识保持最新，比如问“美国总统是谁？”，必须回答是“川普”，如果再回答“奥巴马”就不合适了，讲者提供了一套通过先通过网络热搜找到部分热词，然后再推广到更多热词，然后去更新热词的知识。 CN-DBpedia: A Never-Ending Chinese Knowledge Extraction最后这个报告讲的是根据百度百科的知识构建了知识图谱，主要有以下4个模块：抽取、归一化、填充、更新。其实每一块的方法都比较传统，更新模块用的是上一个报告说的方法，具体基于这个知识图谱的应用等等，可以参见CN-DBpedia 总结 这次的青年学者论坛虽然是第一届，个人感觉办的还是挺不错的。 首先在讲者内容安排上，内容都是循序渐进，相互关联，比如Session 1里关于embedding的内容。 内容覆盖比较全，比如Session 3关于Knowledge Graph，让我一个从来没有接触过KG的人，可以从KG的构建、优化、应用3个方面，都有一定的了解，虽然不够详尽，但也都覆盖到了。 其次是论坛的环境，高端、大气，虽然有点小。论坛是在复旦大数据学院的一个会议室开的，有点像领导开会的地方。还有论坛的茶歇不错，哈哈。 最后，说实话，有很多知识不懂，比如teacher forcing，Adversarial Training等等。发现自己在这个领域走的还是比较浅，也可能是因为我现在做的大多数的中文医学NLP的原因，我甚至还动了想继续读博的念头，哈哈。也许这可以是一条路吧，我会继续探索。]]></content>
      <categories>
        <category>诸家堂</category>
      </categories>
      <tags>
        <tag>Knowledge Graph</tag>
        <tag>NLP</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2Fhello_world%2F</url>
    <content type="text"><![CDATA[终于跳进Blog的坑 基于Jekyll的Blog搭建其实，拥有一个Blog的想法在脑子里已经很久了，但是并没有学过前端等等，只会写点python，想过用Django或者其他python web框架，但是一直没有时间做。一个偶然的机会看到Github Pages，百度之后发现也许这是个合适的时机和方法了。本来在github上搜到了Hux的主题，这个是基于Jekyll的，查了一些资料，感觉自己修改起来会比较方便，于是就拿过来用了。配置还是比较简单，直接按照作者的readme来就好，在电脑上架好模板之后，就是自己定制的一些东西了，主要有以下几点： 修改图片 模板是比较文艺、简洁的，在Unsplash找了几张喜欢的图替换上，修改了ico，修改了模板作者的头像，换上自己的挫照。 修改页面图标 因为不玩知乎等等，所以直接把footer里的图标改成了RSS和e-mail，也改了一下copyright等的写法。 增加Categories页面 在看其他blog的时候，发现了大部分的Blog都有Categories页面，于是参考Bruce Zhao’s Blog加入Categories页面。 增加Search 也是在看别人的Blog时(哈哈，自己做东西也要参考一下别人的嘛)发现有很多Blog都有Search的功能，我也觉得这是必要的，本想参照Bruce Zhao’s Blog直接加一个Search页面，但是觉得不方便，后来看到码志，觉得在侧边栏加Search就好，又觉得码志的不太简洁，于是结合以上两个Blog，在侧边栏加入Bruce Zhao’s Blog风格的Search。 增加back-to-top按钮 个人觉得回到top的功能还是比较重要的，所以查了一些方法，加入成功。 Home页面增加缩略图 觉得原作者只展示缩略文字比较单调，想加入缩略图来丰富一下主页，自己查了查加上了，感觉还行。 针对移动端设备优化显示效果 主要是上一条加入缩略图之后，在手机页面上比较难看，后来发现原作者也是对网页在移动端展示做了优化，比如移动端是不展示侧边栏的，于是找了源代码，参照着把首页缩略图以及侧边栏Search进行了同样的优化，比较满意。 注册域名 自己在万网上注册了一个域名oyeblog.com，然后找了重定向教程，把域名绑定，到这一步总算是把Blog搭好。 到上一步，其实已经把Blog搭得差不多了。但是谁叫我爱折腾呢，在网上看到一个Blog的相册特别好看，然后心痒痒想自己也搞一个，毕竟咱也是半个文艺青年嘛。然后开始查基于Jekyll的相册模块，实在没有我喜欢的，想移植又没有技能，最后转向Hexo，因为之前找到的Blog是基于Hexo的，又开始一顿折腾。 转移到Hexo本来想想直接用上文提到的Blog Theme，然后发现他是从ins上直接拿图，而且其他方面又不喜欢。接着开始一轮Hexo的主题搜索，找到一个比较心仪的Theme，也有比较好的相册模块，对移动端阅读支持也比较好，接下来就是根据作者的文档一步一步配置，不得不说作者的文档真的超级详细，超级具体。以下是自己的几点变化： 修改图片 作者的风格是Material，所以一些默认的header图片也是同样风格，但是我还是喜欢之前Unsplash的风格，所以把很多图片都替换了，换了自己的挫照和Logo。 增加图床 在网上搜索的时候发现大部分博主都会把自己的图片上传到图床，以便CDN加速。于是为了长远考虑，我自己也注册了七牛，把相册的图片传上去了。然后在想加入CDN加速的时候，发现域名必须备案才能加速，然而我部署在github上又不能备案，最终只能作罢，不绑定我的域名到CDN加速了。 部署在Coding.net 个人感觉访问比较慢，网上搜了一些加速的方法，发现部署在Coding.net上可以加快访问速度，于是按照网上教程部署了一下，并且绑定了域名。 希望以后可以多写写画画，不浪费自己搭的Bolg 后记经过两种不同框架搭建Blog，发现Hexo的逻辑比Jekyll复杂一点，毕竟有theme模块，但是这样也就移植性也会比较好。其实感觉有了Blog之后，就会有一种小小的使命感，想把东西都放进来，虽然估计也不会有人看，但是毕竟自己做的，有事没事自己刷刷，耗耗流量，哈哈！希望养成记录生活，记录学习，记录工作的好习惯。]]></content>
      <categories>
        <category>自言语</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Life</tag>
        <tag>Jekyll</tag>
      </tags>
  </entry>
</search>
